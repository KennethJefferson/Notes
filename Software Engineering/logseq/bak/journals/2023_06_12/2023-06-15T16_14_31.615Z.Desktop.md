- # [[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow]]
# Chapter 1: The [[Machine Learning]] Landscape
- ---
## What is [[Machine Learning]]?
collapsed:: true
	- Machine Learning is the art & science of programming computers to learn
	  from data.
		- > \[Arthur Samuel\] ML is the field of study that gives computers the
		  > ability to learn without being explicitly programmed.
	- A more formal definition:
		- > \[Tom Mitchell\] A computer program is said to learn from experience
		  > $E$ with respect to some task $T$ and some performance measure $P$, if
		  > its performance on $T$, as measured by $P$, improves with experience
		  > $E$.
## Why use [[Machine Learning]]?
collapsed:: true
	- When building non-learners, we usually follow these steps:
		- We make rules
		- We write an algorithm
		- If the algorithm performs well, we deploy. If not, we go back to step `1`
	- However, if the problem is complex, we\'ll likely endup with a long list of rules that are hard to maintain and scale to other similar problems. An ML system would be much shorter, easier to maintain, and in many cases, more accurate.
	- We can simply train an algorithm on a large dataset, then inspect the algorithm's  [[feature importance]] coefficient to gain a better understanding of the relation between the data & the problem. This is called [[data mining]].
- ---
## Examples of Applications
collapsed:: true
	- ML has many applications, the following are a few notable ones:
	- [[Image Classification]]: typically performed using [[convolutional neural networks]].
	- [[Semantic segmentation]]: the algorithm is trained to classify each
	    pixel in an image, one example of this is brain tumor detection.
	- [[Natural Language Processing]] (NLP): More specifically, text
	    classification, which can be learned using [[RNN]]s, [[CNN]]s, or
	    [[Transformers]].
	- [[Chatbots]]: Involve many NLP tasks such as [[Natural Language Understanding]] (NLU) and Question Answering.
	- Forecasting future revenue: a [[regression]] task that can be tackled using multiple algorithms such as:
		- [[Linear Regression]]
		- [[Polynomial Regression]]
id:: baa50e66-ed17-491d-bffe-a108e7bb73e1
		- [[SVM]]
id:: 61ef5bdd-81d9-430f-9ffe-b46b47965695
		- [[Random Forest]]
		- [[Artificial Neural Networks]]
	- [[Speech recognition]]: this problem can be tackled by recognizing the incoming audio signals using [[RNN]]s, [[CNN]]s or [[Transformers]].
	- Credit card fraud detection: detecting frauds can be solved using [[Supervised Learning]] ([[classification]]) or [[Unsupervised Learning]] ([[anomaly detection]])
	    learning.
	- [[Clustering]]: segmenting clients based on their purchases so we can design targeted & more effective marketing campaigns.
	- [[Dimensionality reduction]]: useful for high-dimensional data visualization and [[cluster analysis]] . It can be solved using algorithms such as [[PCA]] or [[T-SNE]].
	- [[Recommender systems]]: where we can feed in the sequence of client purchases (for example) to an [[artificial neural network]] to predict the next purchase.
- ---
## Types of Machine Learning Systems
	- [[Machine learning algorithms]] can be classified according to the amount of supervision they get during training,
	- There are 4 major types of ML algorithms:
	- ### [[Supervised Learning]]
	  collapsed:: true
		- The training set we feed into the algorithm contains the targets/labels/desired predictions.
			- ### [[Supervised Learning Tasks]]
				- [[Classification]]
					- We are interested in predicting discrete values
						- ex: is the email spam  or not spam .
				- [[Regression]]
					- Deals with continuous target values
						- ex: predict the price of houses in dollars.
		- Some regression-based models are used for classification as well, such as [[Logistic Regression]] which outputs a probability $\in [0,1]$.
			- ### [[Supervised Learning Algorithms]]
				- [[K-nearest Neighbors]]
				- [[Linear Regression]]
				- [[Logistic Regression]]
				- [[Decision Trees and Random Forests]]
				- [[Artificial Neural Networks]]
				- [[Naive Bayes]]
	- ### [[Unsupervised Learning]]
	  collapsed:: true
		- Data is unlabeled
		- The system is trying to learn without a teacher by finding internal structure within the dataset.
		- ### [[Algorithms]]
			- [[Clustering]]
				- [[K-means]]
				- [[DBSCAN]]
				- [[Hierarchical Cluster Analysis]]
			- [[Anomaly Detection]]
				- [[one-class SVM]]
				- [[Isolation Forest]]
				- [[Auto-encoders]]
			- [[Dimensionality Reduction]]: The goal is to compress the data without losing too much information.
				- One way to do it is to merge highly correlated features
				- ### Aspects of [[Dimensionality reduction]]
					- [[Principal Component Analysis]]: [[PCA]]
					- [[t-distributed stochastic Neighbor Embedding]]: [[T-SNE]]
					- [[Autoencoders]]
					- [[Kernel PCA]]
					- [[Local Linear Embedding]] (LLE)
			- [[Association rule learning algorithms]] find interesting relations between attributes
				- [[Apriori]]
				- [[Eclat]]
	- ### [[Semi-supervised Learning]]
	  collapsed:: true
		- Partially-labeled data.
		- The goal is to to use unlabeled data around the labeled data as helpers to solve the task.
		- Most [[semi-supervised learning algorithms]] are a combination of [[Unsupervised Learning Algorithms]] and [[Supervised Learning Algorithms]] .
	- ### [[Reiforcement Learning]]
	  collapsed:: true
		- An agent observes the environment, selects an action, gets a reward, and updates its policy.
		- We can also categorize ML systems to **[[Batch Algorithms]]** or **[[Online Algorithms]]**.
			- The question is is whether the algorithm will learn from an incoming stream of data or not.
	- ### [[Batch Algorithms]] vs. [[Online Algorithms]]
	  collapsed:: true
		- In [[Batch Learning]] , the model is incapable of [[Incremental Learning]],
		  collapsed:: true
			- It starts by learning from all of the available data offline, and then gets deployed to produce predictions without feeding it any new data points.
				- Another name of [[Batch Learning]] is [[Offline Learning]].
		- In [[Online Learning]] , we train the data incrementally by continuously feeding it data instances as they come.
		  collapsed:: true
			- Individually or in small groups of instances called *mini-batches*.
			- Each learning step is fast and cheap, so the system can learn as data comes, on the fly.
			- Great for systems that receive data in a continuous flow.
			- One important aspect of online learning is how fast the learning algorithm should adapt to new data points or to changes to the overall data distribution.
			- With a big learning rate, the model tends to forget past data and lean heavily towards new data points.
			- With a small learning rate, the model tends to slightly adapt to new data points but keeps its knowledge on old data points mostly intact.
id:: 40eecfac-f742-4332-b2d2-31199497183e
			- ### Challenges
				- Can be damaged with bad incoming data points and clients will notice that on the fly.
					- To mitigate this, we can closely monitor the system through performance metrics and turn off online learning or revert back to a previous model state.
					- We have to also make sure we clean the data before feeding it to the model by conducting anomaly/outlier detection.
### Instance-based versus Model-based Learning
- One other way to categorize machine learning algorithms is how they generalize.
	- There are two approaches to generalization:
		- instance-based approaches
		- model-based approaches.
- With instance-based Learning, we perform similarity-based comparisons, a new data point would be classified based on its similarity to the target group in the training set, this would require a measure of similarity.
- In model-based learning we build a model for each class of data points and then use the model to classify a new data point (from the validation/test/production environment).
id:: e9c603a6-d91e-4460-9aef-a3ec6f30dd10
  
  Let\'s go through an example of model-based learning using linear
  regression:
  :::
  
  ::: {.cell .code execution_count="2"}
  ``` python
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns
  import sklearn.linear_model
  ```
  :::
  
  ::: {.cell .code execution_count="3"}
  ``` python
  # URLs for data
  gdp_data_url = 'https://educational-data-samples.s3.amazonaws.com/books/hands-on-ML/WEO_Data.csv'
  oecd_data_url = 'https://educational-data-samples.s3.amazonaws.com/books/hands-on-ML/OECD.csv'
  ```
  :::
  
  ::: {.cell .code}
  ``` python
  # get data & import it as a dataframe
  gdp = pd.read_csv(gdp_data_url, sep='\t', encoding='latin1', thousands=',', na_values='n/a').dropna()
  gdp.head()
  ```
  :::
  
  ::: {.cell .code execution_count="4"}
  ``` python
  # get oecd data & import it as a dataframe
  oecd = pd.read_csv(oecd_data_url, thousands=',')[['Country', 'Indicator', 'Measure', 'Inequality', 'Unit', 'PowerCode', 'Value']]
  oecd.head()
  ```
  
  ::: {.output .execute_result execution_count="4"}
  ```{=html}
  <div>
  <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
  
    .dataframe tbody tr th {
        vertical-align: top;
    }
  
    .dataframe thead th {
        text-align: right;
    }
  </style>
  <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Country</th>
      <th>Indicator</th>
      <th>Measure</th>
      <th>Inequality</th>
      <th>Unit</th>
      <th>PowerCode</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Australia</td>
      <td>Labour market insecurity</td>
      <td>Value</td>
      <td>Total</td>
      <td>Percentage</td>
      <td>Units</td>
      <td>5.4</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Austria</td>
      <td>Labour market insecurity</td>
      <td>Value</td>
      <td>Total</td>
      <td>Percentage</td>
      <td>Units</td>
      <td>3.5</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Belgium</td>
      <td>Labour market insecurity</td>
      <td>Value</td>
      <td>Total</td>
      <td>Percentage</td>
      <td>Units</td>
      <td>3.7</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Canada</td>
      <td>Labour market insecurity</td>
      <td>Value</td>
      <td>Total</td>
      <td>Percentage</td>
      <td>Units</td>
      <td>6.0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Czech Republic</td>
      <td>Labour market insecurity</td>
      <td>Value</td>
      <td>Total</td>
      <td>Percentage</td>
      <td>Units</td>
      <td>3.1</td>
    </tr>
  </tbody>
  </table>
  </div>
  ```
  :::
  :::
  
  ::: {.cell .code execution_count="5"}
  ``` python
  # Get countries + Life satisfaction
  oecd = oecd[['Country', 'Value']][oecd['Indicator'] == 'Life satisfaction']
  oecd = oecd.rename(columns={'Value': 'Life satisfaction'})
  oecd.head()
  ```
  
  ::: {.output .execute_result execution_count="5"}
  ```{=html}
  <div>
  <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
  
    .dataframe tbody tr th {
        vertical-align: top;
    }
  
    .dataframe thead th {
        text-align: right;
    }
  </style>
  <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Country</th>
      <th>Life satisfaction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1812</td>
      <td>Australia</td>
      <td>7.3</td>
    </tr>
    <tr>
      <td>1813</td>
      <td>Austria</td>
      <td>7.1</td>
    </tr>
    <tr>
      <td>1814</td>
      <td>Belgium</td>
      <td>6.9</td>
    </tr>
    <tr>
      <td>1815</td>
      <td>Canada</td>
      <td>7.4</td>
    </tr>
    <tr>
      <td>1816</td>
      <td>Czech Republic</td>
      <td>6.7</td>
    </tr>
  </tbody>
  </table>
  </div>
  ```
  :::
  :::
  
  ::: {.cell .code execution_count="6"}
  ``` python
  # Get countries + GDP
  gdp = gdp[['Country', '2015']]
  gdp = gdp.rename(columns={'2015': 'GDP per capita (USD)'})
  # filter to get a similar dataframe to the book's
  gdp = gdp[gdp['GDP per capita (USD)'] <= 60000]
  gdp.head()
  ```
  
  ::: {.output .execute_result execution_count="6"}
  ```{=html}
  <div>
  <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
  
    .dataframe tbody tr th {
        vertical-align: top;
    }
  
    .dataframe thead th {
        text-align: right;
    }
  </style>
  <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Country</th>
      <th>GDP per capita (USD)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Afghanistan</td>
      <td>599.994</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Albania</td>
      <td>3995.383</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Algeria</td>
      <td>4318.135</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Angola</td>
      <td>4100.315</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Antigua and Barbuda</td>
      <td>14414.302</td>
    </tr>
  </tbody>
  </table>
  </div>
  ```
  :::
  :::
  
  ::: {.cell .code execution_count="7"}
  ``` python
  # Join GDP & Life satisfaction tables + Group by country as the unique column and average-aggregate GDP & Life satisfaction
  country_stats = pd.merge(gdp, oecd, on='Country').groupby(['Country']).mean().reset_index()
  country_stats.head()
  ```
  
  ::: {.output .execute_result execution_count="7"}
  ```{=html}
  <div>
  <style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
  
    .dataframe tbody tr th {
        vertical-align: top;
    }
  
    .dataframe thead th {
        text-align: right;
    }
  </style>
  <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Country</th>
      <th>GDP per capita (USD)</th>
      <th>Life satisfaction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Australia</td>
      <td>50961.865</td>
      <td>7.350</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Austria</td>
      <td>43724.031</td>
      <td>7.225</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Belgium</td>
      <td>40106.632</td>
      <td>7.000</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Brazil</td>
      <td>8669.998</td>
      <td>6.400</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Canada</td>
      <td>43331.961</td>
      <td>7.425</td>
    </tr>
  </tbody>
  </table>
  </div>
  ```
  :::
  :::
  
  ::: {.cell .code execution_count="8"}
  ``` python
  # get GDP for the x-axis and Life Satisfaction for the y-axis
  X = country_stats['GDP per capita (USD)'].values.reshape(-1, 1)
  y = country_stats['Life satisfaction'].values.reshape(-1, 1)
  X.shape, y.shape
  ```
  
  ::: {.output .execute_result execution_count="8"}
    ((37, 1), (37, 1))
  :::
  :::
  
  ::: {.cell .code execution_count="9"}
  ``` python
  # Visualize Data
  country_stats.plot(kind='scatter', x='GDP per capita (USD)', y='Life satisfaction', xlim=(0, 60000), ylim=(0,10))
  plt.show()
  ```
  
  ::: {.output .display_data}
  ![](vertopal_9bd129017a8a4a05be4f355ef6bc4fb5/aa839f5c57ed3a97ca98e6d158a7d1a5e494412d.png)
  :::
  :::
  
  ::: {.cell .code execution_count="10"}
  ``` python
  # Select a Linear Model
  model = sklearn.linear_model.LinearRegression()
  ```
  :::
  
  ::: {.cell .code execution_count="11"}
  ``` python
  # Train the Model
  model.fit(X, y,)
  ```
  
  ::: {.output .execute_result execution_count="11"}
    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
  :::
  :::
  
  ::: {.cell .code execution_count="12"}
  ``` python
  # Make a prediction for Cyprus
  X_new = [[22587]]  # Cyprus GDP per capita
  print(model.predict(X_new))
  ```
  
  ::: {.output .stream .stdout}
    [[6.2910907]]
  :::
  :::
  
  ::: {.cell .code execution_count="16"}
  ``` python
  # let's visualize our model, because it's a linear one, we can plot it using two points
  X = [[10000], [50000]]
  y_hat = model.predict(X)
  ```
  :::
  
  ::: {.cell .code execution_count="17"}
  ``` python
  # Visualize Data
  country_stats.plot(kind='scatter', x='GDP per capita (USD)', y='Life satisfaction', xlim=(0, 60000), ylim=(0,10))
  plt.plot(X, y_hat, c='red')
  plt.show()
  ```
  
  ::: {.output .display_data}
  ![](vertopal_9bd129017a8a4a05be4f355ef6bc4fb5/a3a0888c0d41c7e0b16c064198712c6fafa1f493.png)
  :::
  :::
  
  ::: {.cell .markdown}
  In summary, we go through the following steps to deploy an ML algorithm:
  
  1.  We study the data: Exploratory data analysis (EDA)
  2.  We select the model
  3.  We train the model
  4.  We infer using the model
  :::
  
  ::: {.cell .markdown}
## Main Challenges of Machine Learning

The two things that can go wrong with a machine learning project are:
- Collecting bad data.
- Picking a bad learning algorithm
### Data

Data quantity can be a big problem. Even for very simple ml algorihtms,
it takes thousands of examples for a convolutional neural network to
recognize Cat/Dog in images. A famous paper showed that many algorithms,
ranging from very simple ones to complex, perform relatively the same
when given enough data. The authors argued that companies should
reconsider where to invest their money, in algorithms development or in
data corpus engineering.

The training sample can also be non-representative of the source
dataset. In order to generalize well, It\'s important that our training
data be representative of the data that we want to use in production.
If, for example, the training set is too small, we will have sampling
noise. Even large samples can be non-representative if the sampling
method is flawed, this is called **sampling bias** (Example:
non-response bias).

Poor quality data can also be a big challenge. Obviously, if our
training data is full of outliers, errors, and noise, it will make it
harder for the algorithm to detect the underlying patterns, resulting in
a bad model. It\'s always better to properly clean and investigate the
data before doing any modeling. Examples of data cleaning:
- Outlier detection and cleaning by either removing or replacing the
    outlier values.
- Cleaning missing features by either discarding their instances,
    filling them with median/average, or training an auxiliary model to
    predict their values.
  
  It\'s almost always the case that datasets contain irrelevant features.
  Our system will only learn if the data contains many relevant features
  and not so many irrelevant ones. A critical part of the success of a
  machine learning project is what\'s called **feature engineering** or
  coming up with features that would produce a quality model, it contains
  two steps:
  
  1.  Feature selection: selecting the most useful features to analyze.
  2.  Feature Extraction: adding new features based of the selected ones.
    We can also create new features by gathering new data.
### Overfitting
id:: 9ed71eef-be5c-4d7d-a790-43d730b489a5

Overfitting means that the ML system performs well on the training data
but fails to generalize. Complex models such as deep neural networks
tend to memorize training data noise or even the data sample itself if
it is small enough. We can do the following to mitigate overfitting:
- Select a model with fewer weights/parameters to constrain its
    predictive power so that it can use only the strongest present
    patterns.
- Gather more training data.
- Reduce the noise in the training data by fixing errors and eliminate
    outliers.
  
  Constraining a model & fighting overfitting is called
  **regularization**. If we take the example of a single linear regression
  model ($f(x)=ax+b$), it has two degrees of freedom ($2$ parameters). If
  we let the algorithm change one parameter\'s values freely but have a
  set interval around parameter 2, it will have between 1 and 2 degree of
  freedom. We would want a good balance between keeping the model as
  simple as possible while giving the model the ability to capture out of
  training data patterns.
  
  Regularization can be controlled using the model\'s hyperparameters,
  which describe how the model should learn.
### Underfitting

Underfitting is the opposite of overfitting. It occurs when the model is
too simple to capture the underlying structure of the training data. We
present the following solutions to the problem:
- We can select a more powerful model, with more parameters
- We feed better features to the learning algorithm (feature
    engineering)
- We reduce the constraints on the model (reduce regularization)
  :::
  
  ::: {.cell .markdown}
## Testing & Validating {#testing--validating}

We can evaluate our model by splitting the data into two sets:
**training** and **testing** data sets. We only care about
out-of-training error, or generalization error, as it is representative
of the model\'s performance in a production environment.

If our training error is low but the testing error is high, this means
that the model is overfitting. It\'s common to use **80%** of the data
for training and the remaining 20% for testing, but it is dependent on
the size of the original data set, the bigger it is, the less percent we
can take as a testing set.

If we fine-tune regularization parameters on the test set, we are sort
of overfitting to it, so we need another data set for hyper-parameter
tuning, this data set is usually called the **validation set**. The
validation set should be set aside from the training set. After
conducting hyper-parameter tuning on using the validation set, we train
the model on the full training set (with validation) & evaluate on the
test set.

A simple but computationally expensive solution to setting aside a large
validation set is to perform repeated cross-validation. Its drawback is
that we have to train the model N-repetitions. The validation set & the
test set must be as representative as possible of the data we will use
in production.

One Problem is that if the algorithm is performing poorly on the
validation set, we won\'t know if the cause is overfitting or if the
training set isn\'t good for the task at hand. A solution to this is to
introduce another validation set, called \'train-dev\' set. After
training, we will validate the model on both `train-dev` & validation
sets. If evaluation is good on `train-dev` & bad on `validation`, this
means that the data is not good for the task at hand. If evaluation is
bad on `train-dev` & bad on `validation`, this means overfitting or the
algorithm/overall data is not good.

We should think of a model as a simplified version of the observations.
This simplification is meant to discard noise and capture generalizable
useful patterns in the training and testing datasets. To decide what
information to discard and what to keep, you must make assumptions. For
example, a linear model assumes that the relation between the input &
output is fundamentally linear & the distance between the model line and
the observations is essentially **noise**.

If we make no assumptions about the data, than there is no need to
prefer one model over another. This is the point behind the \"no free
lunch theorem\", which states the following:

> \[David Wolpert & William Macready\] Any two optimization algorithms
> are equivalent when their performance is averaged across all possible
> problems
:::

::: {.cell .markdown}
## Exercices

**1. How would you define Machine Learning?**

Machine Learning is the computer\'s ability to learn from data without
being explicitly programmed.

**2. Can you name four types of problems where it shines?**

Image classification, Voice Recognition, Semantic Segmentation,
Sentiment Analysis.

**3. What is a labeled training set?**

In the context of supervised learning, a labeled training set is a data
set with available targets. Meaning that what you want to predict is
known before training.

Example: a training set composed of images of cats or dogs and the
corresponding **label** (cat/dog) for each of the images.

**4. What are the two most common supervised tasks?**

Classification (where the target is categorical in nature) & Regression
(where the labels\' domain is continuous).

**5. Can you name four common unsupervised tasks?**

Clustering, Anomaly detection, Visualization, dimensionality reduction.

**6. What type of machine learning algorithm would you use to allow a
robot to walk in many paths in an unknown terrain?**

Reinforcement learning.

**7. What type of algorithm would you use to segment your customers into
multiple groups?**

An unsupervised clustering algorithm like `K-means` or `DBSCAN`.

**8. Would you frame the problem of spam detection as a supervised
learning or an unsupervised learning problem?**

It a supervised learning problem.

**9. What is an online learning system?**

An online learning system continues to learn from new data after being
deployed in production, in contrast to a batch learning model which
would stop learning after the initial training process.

**10. What is out-of-core learning?**

We use out-of-core learning algorithms when the training data can\'t fit
in a computer\'s RAM.

**11. What type of learning algorithm relies on a similarity measure to
make predictions?**

Instance-based models, an example of this is K-nearest neighbors.

**12. What is the difference between a model\'s parameters and a
learning algorithm\'s hyper-parameter?**

Model parameters are the knobs that, collectively, store the learned
knowledge of the model, also called weights, they are continuously
changed during training to minimize a cost function. Examples are `a` &
`b` in the linear model: $f(x) = ax + b$.

A learning algorithm\'s hyper-parameter describes how the algorithm
should learn and control the predictive power of the model, example is
the learning rate, number of layers in a NN, number of parameters, batch
size, they are set before training and aren\'t changed during training.

**13. What do model-based algorithms search for?**

A decision boundary.

**14. What is the most common strategy they use to succeed?**

They minimize a cost function that describe the distance between the
predictions outputted by the model and the real target values.

**14. How do they make predictions?**

They start with a set of initial parameters, make predictions based on
the input & their parameters, then adjust their parameters to minimize
cost.

**15. Can you name 4 of the main challenges in machine learning?**

For algorithmic challenges: Model Overfitting, Model Underfitting. Data
challenges include: Data mismatch, Noisy data.

**16. If your model performs great on the training data but fails on the
test data, What is happening?**

Overfitting, the model starts memorizing noise present on the training
data to further minimize the cost function.

**17. Can you name 3 possible solutions?**

Regularization, Adding more data, Simplifying the model.

**18. What is a test set? & Why you would want to use it?**

The whole data set is usually split into training and testing data sets,
we use test data to evaluate the generalizability of the model beyond
the training data set.

**19. What is the purpose of a validation set?**

A validation set is used to fine-tune the models\' hyper-parameters,
what is called manual training. It leaves the test set for the final
evaluation.

**20. What is the `train-dev` set?, When do you use it? & How do you use
it?**

the `train-dev` is a validation set that is taken from a broad training
set after performing the train/validation/test split. We use it when we
have a broad training data set (ex. images of all animals) but specific
validation/test set (zoo animal pics taken with mobile phones) and we
want to correctly interpret the model\'s evaluation.

We train the model on the training data set, we evaluate on `train-dev`
and `validation`, if model performs badly on both, we have an
overfitting case, if the model performs good in `train-dev` and badly on
`validation` we have a data mismatch, we conclude that the learning
doesn\'t generalize to our specific production data.

**21. What can go wrong if you tune hyper-parameters using the test
set?**

We can accidentaly overfit to the test set by manually finding
hyper-parameters that perform well on the test set but doesn\'t
generalize to production data.

------------------------------------------------------------------------
:::