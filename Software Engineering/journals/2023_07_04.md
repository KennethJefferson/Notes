- # [[Operations.Pandas]] [[Data Science]]
  collapsed:: true
	- ## Operations
		- ### Importing [[Pandas]]
		  collapsed:: true
			- ```python
			  import pandas as pd
			  ```
		- ### Creating a [[DataFrame]]
		  collapsed:: true
			- **From a Dictionary**
				- ``` python
				  data ={'col1': [1,2,3],
				         'col2': ['a', 'b', 'c']}
				  df = pd.DataFrame(data)
				  ```
			- **From a CSV file**
				- ``` python
				  df = pd.read_csv('file.csv')
				  ```
		- ### Basic [[DataFrame]] Operations
		  collapsed:: true
			- **Viewing Data**
			  collapsed:: true
				- ``` python
				  df.head() # Display the first $n$ rows (default $n=5$ )
				  df.tail() # Display the last $n$ rows (default $n=5$ )
				  df.shape # Get the dimensions of the DataFrame (rows, columns)
				  df.info() # Get summary information about the DataFrame
				  df.describe() # Generate descriptive statistics of numerical columns
				  ```
			- **Accessing Columns**
			  collapsed:: true
				- ``` python
				  - df['col_name'] # Get a single column by name
				    df[['col1', 'col2']] # Get multiple columns by name
				  ```
			- **Accessing Rows**
			  collapsed:: true
				- ``` python
				  df.loc[row_label] # Get a row by label/index
				  df.iloc[row_index] # Get a row by index (integer)
				  df[start:end] # Get rows by slicing
				  ```
			- **Filtering Data**
			  collapsed:: true
				- ``` python
				  df[df['col'] > 5] # Filter rows based on a condition
				  df.query('col > 5') # Alternative way to filter rows based on a condition
				  ```
			- **Sorting Data**
			  collapsed:: true
				- ``` python
				  df.sort_values('col') # Sort DataFrame by a column
				  df.sort_values('col', ascending=False) # Sort in descending order
				  ```
			- **Adding/Removing Columns**
			  collapsed:: true
				- ``` python
				  df['new_col'] = values # Add a new column df.drop('col_name', axis=1, inplace=True) # Remove a column
				  ```
			- **Missing Data**
			  collapsed:: true
				- ``` python
				  df.isnull() # Check for missing values
				  df.dropna() # Remove rows with missing values
				  df.fillna(value) # Replace missing values with a specific value
				  ```
			- **Grouping and Aggregation**
			  collapsed:: true
				- ``` python
				  df.groupby('col').mean() # Group by a column and calculate the mean df.groupby('col').agg(['mean', 'sum']) # Apply multiple aggregations
				  ```
			- **Joining DataFrames**
			  collapsed:: true
				- ``` python
				  df1.merge(df2, on='col') # Inner join based on a column df1.join(df2, on='col') # Join based on index or column
				  ```
			- **Applying Functions**
			  collapsed:: true
				- ``` python
				  df.apply(function) # Apply a function to each element/column/row df.applymap(function) # Apply a function to each element
				  ```
		- ### Data Cleaning
		  collapsed:: true
			- ``` python
			  df.drop_duplicates() # Remove duplicate rows
			  df.replace(old, new) # Replace values
			  df.rename(columns={'old_name': 'new_name'}) # Rename columns
			  ```
		- ### Handling Dates and Times
		  collapsed:: true
			- ``` python
			  df['datetime'] = pd.to_datetime(df['datetime']) # Convert column to datetime
			  df['month'] = df['datetime'].dt.month # Extract month from datetime 
			  df['dayofweek']=df['datetime'].dt.dayofweek # Extract day of the week
			  ```
		- ### Data Manipulation
		  collapsed:: true
			- **Selecting Rows and Columns**
			  collapsed:: true
				- ``` python
				  - df.loc[row_label, 'col'] # Get a specific value based on row label and column name
				  - df.iloc[row_index, col_index] # Get a specific value based on row index and column index
				  - df.loc[:, 'col'] # Select all rows for a specific column 
				    df.iloc[:, col_index] # Select all rows for a specific column by index
				  ```
			- **Conditional Filtering**
			  collapsed:: true
				- ``` python
				  df[df['col'] > value] # Filter rows based on a condition 
				  df[(df['col1'] > value1) & (df['col2'] < value2)] # Multiple conditions
				  ```
			- **Sorting**
			  collapsed:: true
				- ``` python
				  df.sort_values('col', ascending=False) # Sort DataFrame by a column in descending order
				  df.sort_values(['col1', 'col2']) # Sort by multiple columns
				  ```
			- **Aggregation and Grouping**
			  collapsed:: true
				- ``` python
				  df.groupby('col').mean() # Group by a column and calculate the mean  
				  df.groupby(['col1', 'col2']).sum() # Group by multiple columns and calculate the sum
				  ```
			- **Data Reshaping**
			  collapsed:: true
				- ``` python
				  df.pivot(index='col1', columns='col2', values='col3') # Pivot table  
				  df.melt(id_vars=['col1','col2'], value_vars=['col3', 'col4']) # Convert wide to long format
				  ```
			- **Resampling Time Series Data**
			  collapsed:: true
				- ``` python
				  df.resample('D').mean() # Resample data to daily frequency and calculate the mean
				  df.resample('W').sum() # Resample data to weekly frequency and calculate the sum
				  ```
			- **Rolling Window Functions**
			  collapsed:: true
				- ``` python
				  df['rolling_mean'] = df['col'].rolling(window =3).mean() # Calculate rolling mean
				  df['rolling_sum'] = df['col'].rolling(window=5).sum() # Calculate rolling sum
				  ```
			- **Shifting and Lagging Data**
			  collapsed:: true
				- ``` python
				  df['shifted'] = df['col'].shift(1) # Shift values in a column by a specific number of periods
				  df['lagged'] = df['col'].shift(-2) # Lag values in a column by a specific number of periods
				  ```
			- **Merging and Concatenating DataFrames**
			  collapsed:: true
				- ``` python
				  df_merged = pd.merge (df1, df2, on='col') # Merge DataFrames based on a common column
				  df_concatenated = pd.concat([df1, df2]) # Concatenate DataFrames vertically (along rows)
				  ```
			- **Reshaping Data with Stack and Unstack**
			  collapsed:: true
				- ``` python
				  - df_stacked = df.stack() # Reshape DataFrame from wide to long format (creates a MultiIndex)
				    df_unstacked = df.unstack() # Reshape DataFrame from long to wide format
				  ```
			- **Creating a DateTimeIndex**
			  collapsed:: true
				- ``` python
				  df['datetime'] = pd.to_datetime(df['datetime']) # Convert column to datetime
				  df.set_index('datetime', inplace=True) # Set datetime column as the index
				  ```
			- **Time Zone Conversion**
			  collapsed:: true
				- ``` python
				  df= df.tz_localize('UTC') # Set time zone of DataFrame to UTC
				  df = df.tz_convert('America/New_York') # Convert time zone of DataFrame to a specific time zone
				  ```
			- **Resampling Time Series Data**
			  collapsed:: true
				- ``` python
				  df.resample('D').mean() # Resample data to daily frequency and calculate the mean
				  df.resample('W').sum() # Resample data to weekly frequency and calculate the sum
				  ```
		- ### Data Exporting
		  collapsed:: true
			- ``` python
			  df.to_csv('file.csv', index=False) # Export DataFrame to a CSV file 
			  df.to_excel('file.xlsx', index=False) # Export DataFrame to an Excel file
			  ```
# [[Questions.SQL]]
collapsed:: true
	- What is the SQL server query execution sequence?
	  collapsed:: true
		- FROM -> goes to Secondary files via primary file
		- WHERE -> applies filter condition (non-aggregate column)
		- SELECT -> dumps data in tempDB system database
		- GROUP BY -> groups data according to grouping predicate
		- HAVING -> applies filter condition (aggregate function)
		- ORDER BY -> sorts data ascending/descending
	- What is [[Normalization]] ?
	  collapsed:: true
		- Step by step process to reduce the degree of data redundancy.
		- Breaking down one big flat table into multiple table based on normalization rules. Optimizing the memory but not in term of performance.
		- Normalization will get rid of insert, update and delete anomalies.
		- Normalization will improve the performance of the delta operation (aka. DML operation); UPDATE, INSERT, DELETE
		- Normalization will reduce the performance of the read operation; SELECT
	- What are the three degrees of [[Normalization]] and how is normalization done in each degree?
	  collapsed:: true
		- **INF:**
			- A table is in INF when: All the attributes are single-valued.
			- With no repeating columns (in other words, there cannot be two different columns with the same information).
			- With no repeating rows (in other words, the table must have a primary key).
			- All the composite attributes are broken down into its minimal component.
			- There should be SOME (full, partial, or transitive) kind of functional dependencies between non-key and key attributes.
			- 99% of times, it's usually $it's usually 1NF.
		- **2NF:**
			- A table is in 2NF when: $\bullet$ It is in 1NF.
			- There should not be any partial dependencies so they must be removed if they exist.
		- **3NF:**
			- A table is in 3NF when
			- It is in 2NF
			- There should not be any transitive dependencies so they must be removed is they exist
		- **BCNF:**
		  id:: 64a48ebe-c420-48de-a3ca-413abe05c6dc
			- A stronger form of 3NF so it is also known as 3.5NF
			- We do not need to know much about it. Just know that here you compare between a prime attribute and a prime attribute and a non-key attribute and a non-key attribute.
	- What are the different database objects?
	  collapsed:: true
		- There are total seven database objects (6 permanent database object +1 temporary database object) Permanent DB objects
			- Table
			- Views
			- Stored procedures
			- User-defined Functions
			- Triggers
			- Indexes
			- Temporary DB object
			- Cursors
	- What is [[collation]] ?
	  collapsed:: true
		- Bigdata Hadoop: SQL Interview Question with Answers
		- Collation is defined as set of rules that determine how character data can be sorted and compared. This can be used to compare A and, other language characters and also depends on the width of the characters.
		- ASCII value can be used to compare these character data.
	- What is a [[constraint]] and what are the seven constraints?
	  collapsed:: true
		- Constraint: something that limits the flow in a database.
		- 1. Primary key
		- 2. Foreign key
		- 3. Check
			- Ex: check if the salary of employees is over 40,000
		- 4. Default
			- Ex: If the salary of an employee is missing, place it with the default value.
		- 5. Nullability
			- NULL or NOT NULL
		- 6. Unique Key
		- 7. Surrogate Key
			- mainly used in data warehouse
	- What is a [[Surrogate Key]] ?
	  collapsed:: true
		- 'Surrogate' means 'Substitute'.
		- Surrogate key is always implemented with a help of an identity column.
		  Identity column is a column in which the value are automatically generated by a SQLServer based on the seed value and incremental value.
		- Identity columns are ALWAYS INT, which means surrogate keys must be INT. Identity columns cannot have any NULL and cannot have repeated values. Surrogate key is a logical key.
	- What is a derived column, hows does it work, how it affects the performance of a database and how can it be improved?
	  collapsed:: true
		- The Derived Column a new column that is generated on the fly by applying expressions to transformation input columns.
			- Ex: FirstName + ' ' + LastName AS 'Full name'
		- Derived column affect the performances of the data base due to the creation of a temporary new column.
		- Execution plan can save the new column to have better performance next time.
	- What is a [[Transaction]] ?
	  collapsed:: true
		- It is a set of [[TSQL]] statement that must be executed together as a single logical unit. O Has ACID properties:
			- **Atomicity**:
				- Transactions on the DB should be all or nothing. So transactions make sure that any operations in the transaction happen or none of them do.
			- **Consistency**:
				- Values inside the DB should be consistent with the constraints and integrity of the DB before and after a transaction has completed or failed.
			- **Isolation**:
				- Ensures that each transaction is separated from any other transaction occurring on the system.*
			- **Durability**:
				- After successfully being committed to the RDMBS system the transaction will not be lost in the event of a system failure or error.
		- **COMMIT TRANSACTION (transaction ends):**
			- used to end an transaction successfully if no errors were encountered. All DML changes made in the transaction become permanent.
		- **ROLLBACK TRANSACTION (transaction ends):**
			- used to erase a transaction which errors are encountered. All DML changes made in the transaction are undone.
		- **SAVE TRANSACTION (transaction is still active):**
			- sets a savepoint in a transaction. If we roll back, we can only rollback to the most recent savepoint. Only one save point is possible per transaction. However, if you nest Transactions within a Master Trans, you may put Save points in each nested Tran. That is how you create more than one Save point in a Master Transaction.
	- What are the differences between [[OLTP]] and [[OLAP]]?
	  collapsed:: true
		- [[OLTP]] stands for Online Transactional Processing
			- Normalization Level: highly normalized
			- Data Usage : Current Data (Database)
			- Processing : fast for delta operations (DML)
			- Operation : Delta operation (update, insert, delete) aka DML Terms Used : table, columns and relationships
		- [[OLAP]] stands for Online Analytical Processing
			- Normalization Level: highly denormalized
			- Data Usage : historical Data (Data warehouse)
			- Processing : fast for read operations
			- Operation : read operation (select)
			- Terms Used : dimension table, fact table
	- How do you copy just the structure of a table?
	  collapsed:: true
		- SELECT * INTO NewDB.TBL_Structure
		  FROM OldDB.TBL_Structure
		  WHERE $1=0$-- Put any condition that does not make any sense.
	- What are the different types of [[Joins]]?
	  collapsed:: true
		- **[[INNER JOIN]]** : Gets all the matching records from both the left and right tables based on joining columns.
		- **[[LEFT OUTER JOIN]]**: Gets all non-matching records from left table \& AND one copy of matching records from both the tables based on the joining columns.
		- **[[RIGHT OUTER JOIN]]**: Gets all non-matching records from right table \& AND one copy of matching records from both the tables based on the joining columns.
		- **[[FULL OUTER JOIN]]**: Gets all non-matching records from left table \& all non-matching records from right table \& one copy of matching records from both the tables.
		- **[[CROSS JOIN]]**: returns the Cartesian product.
	- What are the different types of [[Restricted Joins]]?
	  collapsed:: true
		- **SELF JOIN**: joining a table to itself
		- **RESTRICTED LEFT OUTER JOIN**: gets all non-matching records from left side
		- **RESTRICTED RIGHT OUTER JOIN**: gets all non-matching records from right side
		- **RESTRICTED FULL OUTER JOIN**: gets all non-matching records from left table \& gets all nonmatching records from right table.
	- What is a [[sub-query]] ?
	  collapsed:: true
		- It is a [[query]] within a [[query]]
		- Syntax:
			- ``` sql
			  SELECT <column_name> FROM <table_name>
			  WHERE <column_name> IN/NOT IN
			  〈another SELECT statement>
			  )
			  ```
		- Everything that we can do using sub queries can be done using Joins, but anything that we can do using Joins may/may not be done using Subquery.
		- [[sub-query]] consists of an inner query and outer query. Inner query is a SELECT statement the result of which is passed to the outer query. The outer query can be SELECT, UPDATE, DELETE. The result of the inner query is generally used to filter what we select from the outer query.
		- We can also have a subquery inside of another subquery and so on. This is called a nested Subquery. Maximum one can have is 32 levels of nested Sub-Queries.
		- What are the SET Operators?
			- SQL set operators allows you to combine results from two or more SELECT statements.
			- Syntax:
				- ``` sql
				  SELECT Col1, Col2, Col3 FROM T1 〈SET OPERATOR〉
				  SELECT Coll, Col2, Col3 FROM T2
				  ```
			- **Rule 1:**
				- The number of columns in first SELECT statement must be same as the number of columns in the second SELECT statement.
			- **Rule 2:**
				- The metadata of all the columns in first SELECT statement MUST be exactly same as the metadata of all the columns in second SELECT statement accordingly.
			- **Rule 3:**
				- ORDER BY clause do not work with first SELECT statement. O UNION, UNION ALL, INTERSECT, EXCEPT
	- What is a [[derived table]]?
	  collapsed:: true
		- SELECT statement that is given an alias name and can now be treated as a virtual table and operations like [[joins]], [[aggregations]], etc, can be performed on it like on an actual table.
		- Scope is query bound, that is a derived table exists only in the query in which it was defined.
			- ``` sql
			  SELECT temp1.SalesOrderID, temp1.TotalDue FROM
			  ```
			- ``` sql
			  (SELECT TOP 3 SalesOrderID, TotalDue FROM Sales.SalesOrderHeader ORDER BY TotalDue DESC) AS templ LEFT OUTER JOIN
			  ```
			- ``` sql
			  (SELECT TOP 2 SalesOrderID, TotalDue FROM Sales.SalesOrderHeader ORDER BY TotalDue DESC) AS temp2 ON templ.SalesOrderID = temp2.SalesOrderID WHERE temp2.SalesOrderID IS NULL
			  ```
	- What is a [[View]]?
	  collapsed:: true
		- Views are database objects which are virtual tables whose structure is defined by underlying SELECT statement and is mainly used to implement security at rows and columns levels on the base table.
		  One can create a view on top of other views.
		- View just needs a result set (SELECT statement).
		- We use views just like regular tables when it comes to query writing. (joins, subqueries, grouping
- # [[MathML Cleaner]] #Mathpix
  collapsed:: true
	- Rules
		- Replace \# with #
- # [[Logseq]]
  collapsed:: true
	- Autowrap text in codebox
		- ``` css
		  /* Wrap the content inside code blocks in all themes */
		  pre, code {
		    white-space : pre-wrap !important;
		  }
		  ```
-